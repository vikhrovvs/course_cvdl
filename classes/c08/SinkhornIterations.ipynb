{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3f2114-7550-4b05-a459-bae65a8bf23d",
   "metadata": {},
   "source": [
    "# Sinkhorn iterations\n",
    "https://www.youtube.com/watch?v=BfOjrQAhG4M\n",
    "\n",
    "**Disclaimer**. Все написанное ниже -- результаты чтения кода и запуска `log_optimal_transport` на пробных данных + моя интерпретация. Я не нашел хороших комментов или релевантного текста в статье. Не исключено, что тут не все верно написано, стоит читать и проверять внимательно, особенно всякие индексы-оси и.т.д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e36a50-f233-4bf4-bdbb-d05242978dab",
   "metadata": {},
   "source": [
    "## Формулировка\n",
    "Пусть даны два набора ключевых точек (kp): набор $n$ размера N (например синий), и набор $m$ размера M (например красный).\n",
    "\n",
    "Для данной матрицы схожести ключевых точек $ S_{ij}[N, M] = (f^n_{i}, f^m_{j}) $ и данного параметра $\\alpha_d$ (dustbin) надо найти матрицу неотрицательных $P_{ij}[N+1, M+1]$ такую что:\n",
    "1. максимальна $ C = \\sum_{i,j}^{N+1, M+1} P_{ij} \\hat{S_{ij}}$, где $\\hat{S}[N+1, M+1]$ (coupling в коде) - это $S[N, M]$ расширенная(конкатенированная) строкой и столбцом, заполненными значением $\\alpha_d$  \n",
    "1. для любых i <= N выполняется: $\\sum_{j}^{M+1}P_{ij} = 1$ (каждый i-й(синий) kp из $n$ \"перетекает\" с коэффициентами $P_{ij}$ в j-е kp из $m$ и в m-dustbin)\n",
    "1. для любых j <= M выполняется: $\\sum_{i}^{N+1}P_{ij} = 1$ (каждый j-й(красный) kp из $m$ \"перетекает\" с коэффициентами $P_{ij}$ в i-е kp из $n$ и в n-dustbin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2951ed-4be2-4ca1-8534-9c17355530fc",
   "metadata": {},
   "source": [
    "\n",
    "## Расширенная формулировка с учетом dustbin\n",
    "Пункты выше - стандартные для оптимального транспорта, но у нас есть еще dustbin-ы, которые надо учесть отдельно. Для реальных kp ограничения по перетеканию строгие - т.е. сумма вероятностей должна быть **равна** 1. Для dustbin ограничения нестрогие по смыслу : в m-dustbin может попасть от 0 до N kp, в n-dustbin может попасть от 0 до M kp. Проблема в том, что метод множителей Лагранжа предполагает, что ограничения заданы строгими равенствами (хотя вообще есть расширения метода на неравенства). Поэтому авторы, судя по коду, делают такой трюк - они задают следующие **строгие** ограничения для каждого dustbin:\n",
    "1. $\\sum_{i}^{N+1}P_{i,M+1} = N$\n",
    "1. $\\sum_{j}^{N+1}P_{N+1,j} = M$\n",
    "\n",
    "На первый взгляд кажется, что это какая-то фигня: получается что N синих kp и M красных kp должны быть отправлены в соответствующие dustbin, т.е. вообще все ключевые точки должны быть отправлены в dustbin. Но так не происходит из за последнего (правого углового) элемента матрицы $P_{N+1,M+1}$, который по смыслу не соответствует ни матчингу какой-то реальной точки с другой точкой, ни матчингу реальной точки с dustbin. Этот элемент участвует только в нормировке dustbin, потому может принимать любое значение - можно считать его численной заглушкой. После итераций его значение всегда получается равным сумме вероятностей всех сматченных точек. Можно визуализировать получающееся решение после итераций такой блок-матрицей (указаны суммы значений элементов блоков, а не сами элементы; KP - ключевые точки, DB - dustbin):\n",
    "\n",
    "KP | Kp N        || DB\n",
    "---|-------------||---\n",
    "KpM|     P       || M - P \n",
    "   |             ||\n",
    "DB |  N - P      || P\n",
    "\n",
    "Сумма всех реальных сматченных точек = P, это примерно количество матчей. Сумма всех элементов матрицы = M+N **всегда** (для любого P) - это количество kp. Сумма (количество) всех реальных несматченных точек - (N-P) и (M-P) соответственно, что логично. Сумма каждого из dustbin N и M соответственно. Вся \"структура\" матрицы определяется всего одной переменной P(количество матчей) - поэтому матрица-решение такого вида существует всегда. Например:\n",
    "1. Если дано N = M точек, и все сматчены, то P=N=M (количество матчей), а N-P=M-P=0 (количество отправленных в dustbin)\n",
    "1. Если даны N и M точек, и никто ни с кем не с матчен, то P=0, N-P=N, M-P=M\n",
    "\n",
    "В итоге довольно странные ограничения на dustbin приводят к логичному решению с более-менее интерпретируемыми свойствами. Интересно, как до этого додумались авторы, имхо это все ни разу не очевидно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adcfa6-6cdf-41ba-b849-a7b114af527c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## [Лагранжиан](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%BD%D0%BE%D0%B6%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%B9_%D0%9B%D0%B0%D0%B3%D1%80%D0%B0%D0%BD%D0%B6%D0%B0)\n",
    "Лагранжиан состоит из четырех частей:\n",
    "1. Максимизируемая сумма \"схожести\"\n",
    "1. Добавка из Sinkhorn Iterations $P \\cdot log(P)$. Возьмем ее с коэффициентом 1 для простоты, могли бы умножить на 0.1 или 0.42 - влияет на \"сглаженность\" оптимального решения.\n",
    "1. Ограничения реальных матчей: \"по i\" (для синих), множители Лагранжа $l^{u}$, в коде $u = l^u$, \"по j\" (для красных), множители Лагранжа $l^{v}$, в коде $v = l^v$\n",
    "1. Ограничения для dustbin (записываем их сначала отдельно для лучшей читаемости). Множители лагранжа для них хранятся как последний элемент $l^{u}_{N+1}$ и $l^{v}_{M+1}$, аналогично тому как сделано в коде.  \n",
    "\n",
    "Получается:\n",
    "$$ L(P_{ij}[N+1, M+1], l^{u}_{i}[N+1], l^{v}_{j}[M+1]) = $$\n",
    "$$\\sum_{i,j}^{N+1, M+1} P_{ij} \\hat{S_{ij}} +   \n",
    "\\sum_{i,j}^{N+1, M+1} P_{ij} (log(P_{ij}) - 1) + \\newline\n",
    "\\sum_{i}^{N} l^{u}_{i} (\\sum_{j}^{M+1}P_{ij} - 1) + \\sum_{j}^{M} l^{v}_{j} (\\sum_{i}^{N+1}P_{ij} - 1) + \\newline\n",
    "l^{u}_{N+1} (\\sum_{j}^{M+1}P_{N+1,j} - M) + l^{v}_{M+1} (\\sum_{i}^{N+1}P_{i,M+1} - N)\n",
    "$$ \n",
    "\n",
    "Для удобства запишем две последних части в одну: обозначим как $\\mu[N+1]$ и $\\nu[M+1]$ требуемые значения сумм строк и столбцов $P_{ij}$ - это 1 везде, кроме последних N+1 и M+1 элементов - там стоят M и N соответственно. Также перенесем множители Лагранжа под суммирование по i, j. Тогда запись чуть короче будет:\n",
    "\n",
    "$$ L(P_{ij}[N+1, M+1], l^{u}_{i}[N+1], l^{v}_{j}[M+1]) = \\newline \\sum_{i,j}^{N+1, M+1}  \n",
    " [ P_{ij} \\hat{S_{ij}} + P_{ij} (log(P_{ij}) - 1) + l^{u}_{i} (P_{ij} - \\mu_{i}) + l^{v}_{j} (P_{ij} - \\nu_{j}) ]\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee562c5d-0731-4f31-9ef1-668e22823bf1",
   "metadata": {},
   "source": [
    "## Вывод уравнений для итераций\n",
    "#### Производная по $P_{ij}$\n",
    "Необходимым условием экстремума является равенство нулю каждой производной $dL/P_{ij}$, а также равенства нулю производных $dL/dl^{u}_{i}$, $dL/dl^{v}_{j}$.\n",
    "Производные по множителям Лагранжа дадут снова исходные ограничения c $\\mu$ и $\\nu$.\n",
    "\n",
    "Производная по $P_{ij}$:\n",
    "$$\n",
    "dL/dP_{ij} = [ \\hat{S_{ij}} + (log(P_{ij}) - 1) + P_{ij} / P_{ij} + l^{u}_{i} + l^{v}_{j} ] = \\newline\n",
    " = \\hat{S_{ij}} + log(P_{ij}) + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "Приравниваем 0, выражаем log(P)\n",
    "\n",
    "$$\n",
    "log(P_{ij}) = -\\hat{S_{ij}} - l^{u}_{i} - l^{v}_{j}\n",
    "$$\n",
    "В таком решении получилось, что чем **больше** $S_{ij}$, тем **меньше** $P_{ij}$, что нелогично - мы же максимизируем сумму, если две точки схожи по S - вероятность матча высокая. Метод Лагранжа ищет не максимум или минимум, а все точки экстремума, которые не меняются, если домножить на -1. Обычно этого не делается, т.к. решается задача минимизации стоимости, а не максимизации схожести.\n",
    "\n",
    "Чтобы решение имело более \"интуитивный\" смысл - домножим (мысленно) в исходном лагранжиане первую часть на -1, тогда получится почти такой же результат с точностью до знака при S.\n",
    "Для множителей Лагранжа знак не имеет значения (с точностью до обозначения это одно и то же). Чтобы получить те же знаки, что и у авторов в коде - \"инвертируем\"(домножим на -1) и их тоже. \n",
    "$$\n",
    "log(P_{ij}) = \\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "\n",
    "В довесок еще обозначим $ Z_{ij} = log(P_{ij}) $ (т.е. $P_{ij} = exp(Z_{ij})$). Используем такую замену (по анологии с кодом) по двум причинам:\n",
    "1. В логарифмическом представлении стабильнее вычисления\n",
    "1. В коде в качестве лосса используется кросс-энтропия, которая принимает на вход не вероятности (значения от 0 до 1), а **логиты**(значения от -inf до 0) \n",
    "\n",
    "<font color='#dd33dd'>\n",
    "$$\n",
    "Z_{ij} =  log(P_{ij}) = \\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j}\n",
    "$$\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1224e-67f5-424e-948f-f856f19af6ef",
   "metadata": {},
   "source": [
    "#### Производная по $l^u, l^v$\n",
    "Производные по $l^u, l^v$ тоже должны быть равны 0, они просто дают исходные ограничения (при вычислении надо правильно учесть внешнюю сумму, лучше видно из длинной записи Лагранжиана):\n",
    "\n",
    "$dL/dl^{u}_i = \\sum_{j}^{M+1}P_{ij} - \\mu_i = 0$\n",
    "\n",
    "$dL/dl^{v}_j = \\sum_{i}^{N+1}P_{ij} - \\nu_i = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d3c6b-90ee-4057-8dc7-907c9a4e36fe",
   "metadata": {},
   "source": [
    "#### Log-space\n",
    "Прологарифмируем $dL/dl^{u}_i$, и используем замену $P_{ij} = exp(Z_{ij}) $.\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(Z_{ij})) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(\\hat{S_{ij}} + l^{u}_{i} + l^{v}_{j})) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "$$\n",
    "log ( \\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{u}_{i})\\cdot exp(l^{v}_{j}))) = log(\\mu_i)\n",
    "$$\n",
    "\n",
    "Суммирование здесь только по j, часть с i можно вынести из под суммы:\n",
    "$$\n",
    "log (exp(l^{u}_{i}) \\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{v}_{j}))) = log(\\mu_i)\n",
    "$$\n",
    "По свойству логарифма log(ab) = log(a) + log(b), log(exp(x)) = x:\n",
    "$$\n",
    "l^{u}_{i} + log(\\sum_{j}^{M+1}exp(\\hat{S_{ij}})\\cdot exp(l^{v}_{j}))) = log(\\mu_i) \\newline\n",
    "$$\n",
    "\n",
    "\n",
    "<font color='#33dd33'>\n",
    "$$\n",
    "l^{u}_{i} = log(\\mu_i) - log(\\sum_{j}^{M+1}exp(\\hat{S_{ij}} + l^{v}_{j}))) \n",
    "$$\n",
    "</font> \n",
    "Точно также для $l^{v}_{j}$:\n",
    "\n",
    "<font color='#33dd33'>\n",
    "$$\n",
    "l^{v}_{j} = log(\\nu_j) - log(\\sum_{i}^{N+1}exp(\\hat{S_{ij}} + l^{u}_{i}))) \n",
    "$$\n",
    "</font> \n",
    "\n",
    "# Зеленые уравнения выше - Sinkhorn Iterations в log-space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d85b31e-29eb-43f6-a318-e79830ec0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "Вызов в модуле:\n",
    "\n",
    "        scores = torch.einsum('bdn,bdm->bnm', mdesc0, mdesc1)\n",
    "        scores = scores / self.config['descriptor_dim']**.5\n",
    "\n",
    "        # Run the optimal transport.\n",
    "        scores = log_optimal_transport(\n",
    "            scores, self.bin_score,\n",
    "            iters=self.config['sinkhorn_iterations'])\n",
    "\"\"\"\n",
    "\n",
    "def log_optimal_transport(scores, alpha, iters: int):\n",
    "    \"\"\" Perform Differentiable Optimal Transport in Log-space for stability\"\"\"\n",
    "    # scores - матрица S[B, N, M]\n",
    "    b, m, n = scores.shape # [B, N, M]\n",
    "    one = scores.new_tensor(1) # scalar 1\n",
    "    ms, ns = (m*one).to(scores), (n*one).to(scores) # scalar N, scalar M \n",
    "    # dustbins: alpha x shape\n",
    "    bins0 = alpha.expand(b, m, 1) \n",
    "    bins1 = alpha.expand(b, 1, n)\n",
    "    alpha = alpha.expand(b, 1, 1)\n",
    "    \n",
    "    # [B, N + 1, M + 1], она же S с крышкой\n",
    "    couplings = torch.cat([torch.cat([scores, bins0], -1),\n",
    "                           torch.cat([bins1, alpha], -1)], 1)\n",
    "    \n",
    "    # Как я понял, здесь norm - это еще один трюк для стабилизации.\n",
    "    # norm = -log(M+N), и сначала он добавляется в log_mu, log_nu,\n",
    "    # а потом вычитается из Z.\n",
    "    # Я протестировал, что если заменить norm на log(1), т.е. на 0\n",
    "    # то все продолжает работать\n",
    "    norm = - (ms + ns).log() # можно заменить на one.log()\n",
    "    \n",
    "    # [ -log(M+N) ... x M ... -log(M+N), log(M) - log(M+N)] [M+1] \n",
    "    # В последний элемент прибавляется log(M) или log(N)\n",
    "    # Если norm=log(1) = 0, то вектор равен\n",
    "    # [log(1), log(1) ..., log(M)]\n",
    "    log_mu = torch.cat([norm.expand(m), ns.log()[None] + norm])\n",
    "    log_nu = torch.cat([norm.expand(n), ms.log()[None] + norm])\n",
    "        \n",
    "    # бродкаст на все батчи\n",
    "    log_mu, log_nu = log_mu[None].expand(b, -1), log_nu[None].expand(b, -1)\n",
    "    # выполняем iters итераций, получаем логарифм(!) вероятности матча\n",
    "    Z = log_sinkhorn_iterations(couplings, log_mu, log_nu, iters)\n",
    "    # откатываем norm обратно (вычитаем -log(M+N))\n",
    "    Z = Z - norm  # multiply probabilities by M+N\n",
    "    return Z\n",
    "\n",
    "def log_sinkhorn_iterations(Z, log_mu, log_nu, iters: int):\n",
    "    \"\"\" Perform Sinkhorn Normalization in Log-space for stability\"\"\"\n",
    "    # инициализируем нулями u,v множители Лагранжа\n",
    "    u, v = torch.zeros_like(log_mu), torch.zeros_like(log_nu)\n",
    "    for _ in range(iters):\n",
    "        # torch.logsumexp = torch.log(torch.sum(torch.exp(...)))\n",
    "        u = log_mu - torch.logsumexp(Z + v.unsqueeze(1), dim=2)\n",
    "        v = log_nu - torch.logsumexp(Z + u.unsqueeze(2), dim=1)\n",
    "    return Z + u.unsqueeze(2) + v.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b313905-c973-4232-a446-e21e865135b9",
   "metadata": {},
   "source": [
    "## Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f4f0508-e0c1-410c-aa13-b5bf577f2033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3978,  2.0107, -2.6327,  1.1628, -0.7444],\n",
       "         [-1.6935, -1.7384, -2.9961, -4.4853, -0.2840],\n",
       "         [-1.1089, -3.1430, -1.7885, -0.7323,  0.0943]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (torch.randn((1, 3, 5)) - 0.5) * 2\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901a7e2f-5030-4f07-8e77-283877dc0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0.00042])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f05019fb-9183-4790-8151-ba6f5bc761a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  6.,  44.,   1.,  24.,   3.,  18.],\n",
       "        [  7.,   4.,   2.,   0.,  19.,  71.],\n",
       "        [  9.,   1.,   5.,  11.,  22.,  55.],\n",
       "        [ 78.,  51.,  92.,  65.,  56., 156.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(torch.exp(log_optimal_transport(t, a, 2)) * 100).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b53862-594a-4ed0-b99a-dfba93aac214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 1.01, 0.99, 1.  , 1.  , 3.  ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сумма по 1 оси\n",
    "# Сумма dustbin = N\n",
    "torch.exp( # Матрица возводится в экспоненту!\n",
    "    log_optimal_transport(t, a, 2)\n",
    ").numpy().round(2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa9e0bdf-7a1e-4e27-b7a3-90aed34ad21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98      , 0.97999996, 0.8       , 5.25      ]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сумма по 2 оси\n",
    "# Сумма dustbin = M\n",
    "(torch.exp(log_optimal_transport(t, a, 2))).numpy().round(2).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "776a6117-b5fe-4438-833e-1b5a31939987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.01"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сумма всей матрицы\n",
    "# M + N\n",
    "(torch.exp(log_optimal_transport(t, a, 2))).numpy().round(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ad50622-10dd-4ddf-97c5-cc03681d207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.04, 0.07, 0.47, 0.21],\n",
       "        [0.06, 0.35, 0.  , 0.17, 0.  , 0.25],\n",
       "        [0.  , 0.  , 0.02, 0.41, 0.01, 0.09],\n",
       "        [0.94, 0.64, 0.94, 0.36, 0.53, 2.45]]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Результаты при 1 итерации\n",
    "torch.exp(log_optimal_transport(t, a, 1)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92f5a82b-3677-4f96-b452-6f897c0c6f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.01, 0.06, 0.06, 0.58, 0.3 ],\n",
       "        [0.09, 0.45, 0.  , 0.13, 0.  , 0.33],\n",
       "        [0.  , 0.  , 0.05, 0.62, 0.01, 0.23],\n",
       "        [0.91, 0.55, 0.89, 0.19, 0.41, 2.14]]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Результаты при 3 итерациях (разница большая с iters=1)\n",
    "torch.exp(log_optimal_transport(t, a, 3)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "076b92ea-f08f-4b4e-8f4e-a4eab14fca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.01, 0.06, 0.05, 0.58, 0.3 ],\n",
       "        [0.09, 0.45, 0.  , 0.12, 0.  , 0.33],\n",
       "        [0.  , 0.  , 0.06, 0.66, 0.01, 0.27],\n",
       "        [0.91, 0.54, 0.88, 0.17, 0.4 , 2.1 ]]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Результаты при 10 итерациях (разница не оч большая с iters=3)\n",
    "torch.exp(log_optimal_transport(t, a, 10)).numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f740f11-a745-4957-b23a-d8fe5da206c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584663f-b4b9-49ac-860e-9e940bfafbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
