{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2220571-43fc-4ff0-adac-5973b1312ae2",
   "metadata": {},
   "source": [
    "## Занятие 10. Распознавание действий на видео\n",
    "\n",
    "### Часть 1. Пакеты Open-MMLab\n",
    "\n",
    "Фреймворк open-mmlab - набор пакетов для решения задач компьютерного зрения от Multimedia Lab в The Chinese University of Hong Kong.\n",
    "- https://github.com/open-mmlab/\n",
    "- https://openmmlab.com/\n",
    "\n",
    "Наиболее известный пакет фреймворка - [mmdetection](https://github.com/open-mmlab/mmdetection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142dd5e-6d46-46ed-91df-ba62e2a19228",
   "metadata": {},
   "source": [
    "Пакеты open-mmlab не являются Python-пакетами в стандартном смысле - помимо Python-кода моделей и чтения данных, в него входят скрипты для обучения, распределенного обучения, тестирования, скачивания данных, конвертации моделей, анализу качества работы и производительности\n",
    "\n",
    "Авторы стремятся в работе:\n",
    "- к воспроизводимости и документированности результатов\n",
    "- к модульности моделей, расширяемости фреймворка новыми моделями\n",
    "- к самодостаточности фреймворка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427335-5917-4ada-bcb7-1dbcd641037e",
   "metadata": {},
   "source": [
    "#### 1.1 Конфиги\n",
    "Для задания пайплайнов используются python-модули с описанием \"всего\" в виде словарей.\n",
    "\n",
    "\n",
    "Пример: \n",
    "- пайплайн CenterNet https://github.com/open-mmlab/mmdetection/blob/master/configs/centernet/README.md\n",
    "- код головы CenterNet https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/centernet_head.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd42c32-f76b-432b-aef9-8fe6deecf7c6",
   "metadata": {},
   "source": [
    "### Часть 2. MMAction2 и анализ видео\n",
    "https://github.com/open-mmlab/mmaction2 - фреймворк для анализа видео (распознавания действий на видео).\n",
    "\n",
    "Распознавание действий на видео (Action Recognition / Video Understanding) - общая задача со множеством вариаций.\n",
    "\n",
    "Постановка задачи зависит от определения \"действия\", например:\n",
    "1. У действия есть границы во времени ($T_{start}, T_{end}$) - или оно длится от начала до конца видео?\n",
    "2. Действие на видео одно - или их много?\n",
    "3. Действие имеет просранственные характеристики (Bounding Box, Mask) - или оно происходит \"на всём кадре целиком\"?\n",
    "4. Действия могут пересекаться во времени / в пространстве?\n",
    "\n",
    "...\n",
    "\n",
    "Дальше - больше:\n",
    "- действие описывается единственным фиксированным классом (Dance), несколькими классами (Actor / Action), текстом в свободной форме?\n",
    "- есть ли другие модальности - например, аудио?\n",
    "- камера одна или несколько?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e07a66-ba2d-4124-87b8-ef01c1ea6557",
   "metadata": {},
   "source": [
    "### Примеры задач\n",
    "\n",
    "- Action Recognition: на видео - одно действие фиксированного класса без границ по времени и пространству\n",
    "- Activity Localization: на видео одно или несколько действий фиксированных классов с границами по времени\n",
    "- Spatio-Temporal Action Detection = Action Localization + детекции объектов\n",
    "- Action Segmentation: на каждом кадре видео не более одного действия\n",
    "- Action (Event) Spotting: на видео есть отдельные кадры-действия (у действия нет длительности)\n",
    "\n",
    "Часть задач поддерживается [mmaction2](https://github.com/open-mmlab/mmaction2#supported-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c00c3c-16a5-475e-b769-dfcbe1538900",
   "metadata": {},
   "source": [
    "### Часть 3.1. Пример использования обученных моделей\n",
    "Запустим предобученную spatio-temporal action detection на двух примерах.\n",
    "- [demo](https://github.com/open-mmlab/mmaction2/tree/master/demo#spatiotemporal-action-detection-video-demo)\n",
    "- [cofig](https://github.com/open-mmlab/mmaction2/tree/master/configs/detection/ava)\n",
    "\n",
    "Для запуска демо нужно:\n",
    "- `$ pip install mmdet`\n",
    "- `$ pip install moviepy`\n",
    "- на windows не подтянулись CUDA операции в mmcv-full - можно использовать CPU (либо пересобрать mmcv-full с nvcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65542817-a9a2-4442-872a-5122686f6ab0",
   "metadata": {},
   "source": [
    "```\n",
    "(open-mmlab) PS D:\\Repositories\\mmaction2> python demo/demo_spatiotemporal_det.py --video D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\ntu_sample.avi `\n",
    "    --config configs/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb.py `\n",
    "     --checkpoint https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth `\n",
    "     --det-config demo/faster_rcnn_r50_fpn_2x_coco.py `\n",
    "     --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth `\n",
    "     --det-score-thr 0.9 `\n",
    "     --action-score-thr 0.5 `\n",
    "     --label-map tools/data/ava/label_map.txt `\n",
    "     --predict-stepsize 8 `\n",
    "     --output-stepsize 4 `\n",
    "     --output-fps 6 --device cpu:0 --out-filename D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\ntu_spatiotemp.mp4\n",
    "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
    "Performing Human Detection for each frame\n",
    "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 2/2, 0.4 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth\n",
    "Performing SpatioTemporal Action Detection for each clip\n",
    "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 2/2, 0.4 task/s, elapsed: 4s, ETA:     0sPerforming visualization\n",
    "Moviepy - Building video D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\ntu_spatiotemp.mp4.\n",
    "Moviepy - Writing video D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\ntu_spatiotemp.mp4\n",
    "\n",
    "Moviepy - Done !\n",
    "Moviepy - video ready D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\ntu_spatiotemp.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "52d1b34b-9879-4eec-bd7d-4133756a8aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/ntu_spatiotemp.mp4\" controls  width=\"1024\"  height=\"576\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/ntu_spatiotemp.mp4\", width=1024, height=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951ab0c-dcf9-460c-a93b-5ebb808b87dc",
   "metadata": {},
   "source": [
    "```\n",
    "(open-mmlab) PS D:\\Repositories\\mmaction2> python demo/demo_spatiotemporal_det.py --video D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\losash_tem.mp4 `\n",
    ">>     --config configs/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb.py `\n",
    ">>     --checkpoint https://download.openmmlab.com/mmaction/detection/ava/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth `\n",
    ">>     --det-config demo/faster_rcnn_r50_fpn_2x_coco.py `\n",
    ">>     --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth `\n",
    ">>     --det-score-thr 0.9 `\n",
    ">>     --action-score-thr 0.5 `\n",
    ">>     --label-map tools/data/ava/label_map.txt `\n",
    ">>     --predict-stepsize 8 `\n",
    ">>     --output-stepsize 4 `\n",
    ">>     --output-fps 6 --device cpu:0 --out-filename D:\\edu\\teach\\course_cvdl\\classes\\c10\\data\\losash_tem_spatiotemp.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119bc746-c8f1-4ccb-904f-b1796c858aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/losash_tem_spatiotemp.mp4\" controls  width=\"1024\"  height=\"576\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/losash_tem_spatiotemp.mp4\", width=1024, height=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe047a-e4e6-4ea7-b508-3e13b8ead4af",
   "metadata": {},
   "source": [
    "### Часть 4. Пример тренировки и тестирования модели\n",
    "\n",
    "В качестве примера для обучения возьмём другую задачу - Action Localization c Boundary Matching Network и воспроизведем ее результаты на датасете [ActivityNet](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md).\n",
    "\n",
    "[config](https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md)\n",
    "\n",
    "Сеть предсказывает proposals (сегменты) для отдельных действий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d100f-7c98-446b-b037-c43b1aa22dd0",
   "metadata": {},
   "source": [
    "#### 4.1. Подготовка датасета ActivityNet\n",
    "Первые two-stage детекторы (R-CNN) работали по принципу:\n",
    "- обучить классификатор изображений\n",
    "- применить классификатор к патчам изображения, сохранить признаки с последнего слоя\n",
    "- обучить на признаках сеть-детектор (Region Proposal Network)\n",
    "\n",
    "Аналогично работает BMN:\n",
    "- обучить классификатор изображений либо видео (2D/3D CNN)\n",
    "- применить классификатор к последовательности кадров, сохранить признаки с последнего слоя\n",
    "- обучить на признаках сеть-детектор (Region Proposal Network)\n",
    "\n",
    "\n",
    "Будем тренировать только последний этап - [воспользуемся](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md#option-1-use-the-activitynet-rescaled-feature-provided-in-this-repo) готовыми фичами:\n",
    "```\n",
    "$ cd mmaction2/tools/data\n",
    "$ bash download_feature_annotations.sh\n",
    "$ bash download_features.sh\n",
    "$ python process_annotations.py\n",
    "```\n",
    "Фичи каждого видео - тензор \\[400, T\\], приведенные усреднением к \\[400, 100\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41723b44-e972-461e-9171-0e5b634e07cd",
   "metadata": {},
   "source": [
    "#### 4.2. Обучение BMN на ActivityNet\n",
    "`$ python .\\tools\\train.py .\\configs\\localization\\bmn\\bmn_400x100_2x8_9e_activitynet_feature.py --gpus=1`\n",
    "\n",
    "Результаты тренировки автоматически сохраняются в work_dirs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b0a87-6d36-4c09-87b5-61336e4367b6",
   "metadata": {},
   "source": [
    "#### 4.3. Тестирование BMN на ActivityNet\n",
    "Правки:\n",
    "- добавить gpu_ids в config\n",
    "\n",
    "`$ python tools/test.py configs/localization/bmn/bmn_400x100_2x8_9e_activitynet_feature.py .\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\latest.pth --eval AR@AN --out .\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\results_test.json --gpu-collect`\n",
    "\n",
    "```\n",
    "auc: 66.9812\n",
    "AR@1: 0.3360\n",
    "AR@5: 0.4958\n",
    "AR@10: 0.5620\n",
    "AR@100: 0.7503\n",
    "```\n",
    "Сравнить с [config](https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab69b0-45ff-4f5c-9a08-f70c806df0a8",
   "metadata": {},
   "source": [
    "### Часть 5. Собственные расширения фреймворка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ae2f2d-cb27-4cc7-a6f0-b9f3d77e36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.models.localizers import BMN\n",
    "from mmaction.models import build_localizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54252816-bb41-4ab9-b8e2-94c2277c55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config mmaction2\\configs\\_base_\\models\\bmn_400x100.py\n",
    "\n",
    "model_config = dict(\n",
    "    type='BMN',\n",
    "    temporal_dim=100,\n",
    "    boundary_ratio=0.5,\n",
    "    num_samples=32,\n",
    "    num_samples_per_bin=3,\n",
    "    feat_dim=400,\n",
    "    soft_nms_alpha=0.4,\n",
    "    soft_nms_low_threshold=0.5,\n",
    "    soft_nms_high_threshold=0.9,\n",
    "    post_process_top_k=100\n",
    ")\n",
    "model = build_localizer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b3e31ab-3cc2-4c56-bc7a-a74a77a531ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.pop('type');\n",
    "model = BMN(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41b86f66-8d51-4e37-ab77-48c6de974260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 36.979 M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num params: {sum(p.numel() for (_, p) in model.named_parameters()) /1e6:1.3f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3780f89b-dbed-447a-888d-d8196c3e947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_raw_feature = torch.rand(1, 400, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd7846-3f5d-459e-9602-65ef47b5a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.runner import load_checkpoint\n",
    "load_checkpoint(model, r\"D:\\Repositories\\mmaction2\\work_dirs\\bmn_400x100_2x8_9e_activitynet_feature\\latest.pth\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34bee6df-c381-455e-9957-ffa1a9f64bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_meta = [\n",
    "    dict(\n",
    "        video_name='v_test',\n",
    "        duration_second=100,\n",
    "        duration_frame=960,\n",
    "        feature_frame=960\n",
    "    )\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(\n",
    "        one_raw_feature,\n",
    "        gt_bbox=None,\n",
    "        video_meta=video_meta,\n",
    "        return_loss=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a039c91-6985-4721-ba08-48cecac03513",
   "metadata": {},
   "source": [
    "### Напишем модификацию - BMN with Action Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "23026be5-7fab-464d-a86a-4a5b8a429df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from mmaction.models.localizers import BMN\n",
    "from mmaction.models.builder import LOCALIZERS\n",
    "\n",
    "\n",
    "#@LOCALIZERS.register_module()\n",
    "class BMNwActionCls(BMN):\n",
    "    \"\"\"\n",
    "    BMN with action classification.\n",
    "    Adds head for action classificationю.\n",
    "    <Head training is not implemented>.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.num_action_classes = kwargs.pop('num_action_classes', 10)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.head_action_cls = nn.Sequential(\n",
    "            nn.Conv1d(self.feat_dim, self.num_action_classes, kernel_size=1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward_test(self, raw_feature, video_meta):\n",
    "        outputs = super().forward_test(raw_feature, video_meta)        \n",
    "        action_cls_pred = self.head_action_cls(raw_feature)\n",
    "        # [batch_size, self.num_action_classes, self.tscale]\n",
    "        batch_size, nc, ts = action_cls_pred.shape\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p, prop in enumerate(outputs[b]['proposal_list']):\n",
    "                t_start, t_end = prop['segment']\n",
    "                proposal_mean_prob = action_cls_pred[b, :, round(t_start):round(t_end)].mean(axis=1)\n",
    "                outputs[b]['proposal_list'][p]['action_cls'] = torch.argmax(proposal_mean_prob)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a91f6f61-6c1b-4f2a-a39a-480fac5c4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wc = BMNwActionCls(num_action_classes=42, **model_config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b96f706-9b8b-4d4b-89bf-298cf9c74e45",
   "metadata": {},
   "source": [
    "model_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31379ee9-4e34-43ec-a184-89aa5992600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_meta = [\n",
    "    dict(\n",
    "        video_name='v_test',\n",
    "        duration_second=100,\n",
    "        duration_frame=960,\n",
    "        feature_frame=960\n",
    "    )\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model_wc(\n",
    "        one_raw_feature,\n",
    "        gt_bbox=None,\n",
    "        video_meta=video_meta,\n",
    "        return_loss=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32d36b55-9d5e-42fd-b149-cb7929a507aa",
   "metadata": {},
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cacf8a7f-426d-4b11-9b6f-eb6d7478c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ec06638b-a092-49e9-b35b-8e1f08bbde83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Repositories\\\\mmaction2\\\\mmaction\\\\models\\\\localizers\\\\bmn_w_ac.py'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile('bmn_w_ac.py', r\"D:\\Repositories\\mmaction2\\mmaction\\models\\localizers\\bmn_w_ac.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98e025-6500-44a0-9479-88e595d7cf96",
   "metadata": {},
   "source": [
    "### Использование модификации\n",
    "- Добавить файл mmaction/models/localizers/bmn_w_ac.py\n",
    "- Добавить импорт модели в mmaction/models/localizers/\\__init__.py (иначе во время запуска скрипт тренировки не найдёт новую модель)\n",
    "```\n",
    "python .\\tools\\train.py .\\configs\\localization\\bmn\\bmn_400x100_2x8_9e_activitynet_feature.py \n",
    "    --gpus=1 \n",
    "    --work-dir=./work_dirs/bmn_400x100_2x8_9e_activitynet_feature_w_ac/ \n",
    "    --cfg-options model.type=BMNwActionCls \n",
    "    --cfg-options data.videos_per_gpu=4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a4f9a-da95-403d-a500-23bcdb120420",
   "metadata": {},
   "source": [
    "### Итоги\n",
    "- есть множество вариаций задач распознавания видео\n",
    "- многие из них имеют baseline решения в пакете mmaction2\n",
    "- у open-mmlab также есть ряд пакетов для других задач компьютерного зрения\n",
    "- решения из пакетов open-mmlab легко воспроизвести и можно модифицировать под свои нужды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cee49b-16ae-4f43-ab20-e7df202b2ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
